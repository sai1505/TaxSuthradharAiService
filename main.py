import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
import os
from dotenv import load_dotenv

# --- Pre-startup Configuration ---
# Load environment variables from a .env file
load_dotenv()

# --- FastAPI App Initialization ---
app = FastAPI(
    title="LangChain Groq Microservice",
    description="An API that interacts with a Groq-powered LLM using LangChain.",
    version="1.0.0",
)

# --- API Key and Model Validation ---
# Securely get the Groq API key from environment variables
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    raise ValueError("GROQ_API_KEY environment variable not set! Please create a .env file and add it.")

# Initialize the ChatGroq model
# We initialize it once here so it's reused across requests for efficiency
try:
    chat_model = ChatGroq(
        temperature=0.7,
        groq_api_key=GROQ_API_KEY,
        model_name="gemma2-9b-it",
    )
except Exception as e:
    raise RuntimeError(f"Failed to initialize ChatGroq model: {e}")


# --- Pydantic Models for Request and Response ---
# Define the structure of the incoming request body for data validation
class PromptRequest(BaseModel):
    text: str = Field(..., min_length=1, description="The text prompt to send to the LLM.")

# Define the structure of the outgoing JSON response
class LLMResponse(BaseModel):
    response: str = Field(..., description="The response generated by the LLM.")


# --- LangChain Prompt and Chain Definition ---
# Create a prompt template. The 'input' variable will be filled by the user's text.
system_prompt = "You are a helpful assistant. Respond to the user's query concisely."
human_prompt = "{input}"
prompt_template = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", human_prompt),
])

# Create the LangChain processing chain
# This chain links the prompt template with the language model
chain = prompt_template | chat_model


# --- API Endpoint Definition ---
@app.post("/invoke", response_model=LLMResponse, summary="Invoke the LLM with a prompt")
async def invoke_llm(request: PromptRequest):
    """
    Receives a text prompt and returns the LLM's response.

    This endpoint uses a LangChain chain with ChatGroq to process the input.
    - **text**: The input string to be sent to the language model.
    """
    try:
        # Use the chain's invoke method to get a response from the LLM
        llm_result = chain.invoke({"input": request.text})

        # The result object has a 'content' attribute with the text response
        response_content = llm_result.content

        if not response_content:
            # Handle cases where the LLM might return an empty response
            raise HTTPException(status_code=500, detail="LLM returned an empty response.")

        return {"response": response_content}

    except Exception as e:
        # Catch any potential errors during the API call and return a server error
        print(f"An error occurred: {e}")
        raise HTTPException(status_code=500, detail=f"An error occurred while processing your request: {e}")


# --- Main Block to Run the App ---
# This makes the script runnable with "python app.py"
if __name__ == "__main__":
    print("Starting FastAPI server...")
    # Uvicorn is an ASGI server that runs our FastAPI application
    uvicorn.run(app, host="127.0.0.1", port=8000)
